{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd0796-d59f-4a80-b498-0f0b2d287d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2049 images belonging to 6 classes.\n",
      "Found 546 images belonging to 6 classes.\n",
      "Training VGG16 Model... ðŸš€\n",
      "Epoch 1/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 4s/step - accuracy: 0.1893 - loss: 1.8918 - val_accuracy: 0.3516 - val_loss: 1.6596\n",
      "Epoch 2/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 4s/step - accuracy: 0.3138 - loss: 1.6517 - val_accuracy: 0.4689 - val_loss: 1.5363\n",
      "Epoch 3/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 4s/step - accuracy: 0.3913 - loss: 1.5520 - val_accuracy: 0.5018 - val_loss: 1.4324\n",
      "Epoch 4/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 4s/step - accuracy: 0.4150 - loss: 1.4666 - val_accuracy: 0.5055 - val_loss: 1.3873\n",
      "Epoch 5/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 4s/step - accuracy: 0.4406 - loss: 1.4222 - val_accuracy: 0.5293 - val_loss: 1.3027\n",
      "Epoch 6/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 4s/step - accuracy: 0.4815 - loss: 1.3393 - val_accuracy: 0.5623 - val_loss: 1.2567\n",
      "Epoch 7/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 4s/step - accuracy: 0.4972 - loss: 1.3056 - val_accuracy: 0.5769 - val_loss: 1.1939\n",
      "Epoch 8/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 4s/step - accuracy: 0.4899 - loss: 1.3100 - val_accuracy: 0.5897 - val_loss: 1.1751\n",
      "Epoch 9/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 4s/step - accuracy: 0.5367 - loss: 1.2433 - val_accuracy: 0.5879 - val_loss: 1.1442\n",
      "Epoch 10/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 4s/step - accuracy: 0.5683 - loss: 1.1367 - val_accuracy: 0.5916 - val_loss: 1.1139\n",
      "Epoch 11/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 4s/step - accuracy: 0.5604 - loss: 1.1868 - val_accuracy: 0.6209 - val_loss: 1.0758\n",
      "Epoch 12/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 4s/step - accuracy: 0.5967 - loss: 1.1059 - val_accuracy: 0.6245 - val_loss: 1.0639\n",
      "Epoch 13/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 4s/step - accuracy: 0.6242 - loss: 1.0439 - val_accuracy: 0.6410 - val_loss: 1.0616\n",
      "Epoch 14/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 4s/step - accuracy: 0.6005 - loss: 1.0382 - val_accuracy: 0.6007 - val_loss: 1.0372\n",
      "Epoch 15/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 4s/step - accuracy: 0.6146 - loss: 1.0137 - val_accuracy: 0.6630 - val_loss: 0.9794\n",
      "Epoch 16/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 4s/step - accuracy: 0.6369 - loss: 0.9704 - val_accuracy: 0.6319 - val_loss: 0.9849\n",
      "Epoch 17/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 4s/step - accuracy: 0.6711 - loss: 0.9464 - val_accuracy: 0.6484 - val_loss: 0.9594\n",
      "Epoch 18/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 4s/step - accuracy: 0.6299 - loss: 0.9678 - val_accuracy: 0.6392 - val_loss: 0.9570\n",
      "Epoch 19/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 3s/step - accuracy: 0.6773 - loss: 0.8776 - val_accuracy: 0.6612 - val_loss: 0.9306\n",
      "Epoch 20/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 4s/step - accuracy: 0.6727 - loss: 0.8894 - val_accuracy: 0.6612 - val_loss: 0.9242\n",
      "Epoch 21/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 4s/step - accuracy: 0.6704 - loss: 0.9012 - val_accuracy: 0.6667 - val_loss: 0.9095\n",
      "Epoch 22/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 4s/step - accuracy: 0.6631 - loss: 0.8747 - val_accuracy: 0.6813 - val_loss: 0.9021\n",
      "Epoch 23/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 4s/step - accuracy: 0.6901 - loss: 0.8503 - val_accuracy: 0.6905 - val_loss: 0.8995\n",
      "Epoch 24/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 4s/step - accuracy: 0.6966 - loss: 0.8540 - val_accuracy: 0.6685 - val_loss: 0.8842\n",
      "Epoch 25/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 3s/step - accuracy: 0.7020 - loss: 0.8123 - val_accuracy: 0.6868 - val_loss: 0.8781\n",
      "Epoch 26/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 4s/step - accuracy: 0.7020 - loss: 0.8262 - val_accuracy: 0.6868 - val_loss: 0.8788\n",
      "Epoch 27/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 4s/step - accuracy: 0.7059 - loss: 0.7978 - val_accuracy: 0.7015 - val_loss: 0.8546\n",
      "Epoch 28/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 4s/step - accuracy: 0.6961 - loss: 0.8261 - val_accuracy: 0.6960 - val_loss: 0.8653\n",
      "Epoch 29/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 4s/step - accuracy: 0.7185 - loss: 0.8248 - val_accuracy: 0.6905 - val_loss: 0.8331\n",
      "Epoch 30/30\n",
      "\u001b[1m65/65\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 4s/step - accuracy: 0.6986 - loss: 0.7917 - val_accuracy: 0.6905 - val_loss: 0.8360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VGG16 Model saved as 'vgg16_model.h5'.\n",
      "âœ… VGG16 Model Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import threading  # Used for running classification in the background\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Define Paths & Parameters\n",
    "# --------------------------- #\n",
    "train_dir = \"output/train\"\n",
    "val_dir = \"output/val\"\n",
    "input_shape = (224, 224, 3)\n",
    "batch_size = 32\n",
    "NUM_CLASSES = 6 # Updated number of traffic sign classes\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Data Preparation (for VGG16)\n",
    "# --------------------------- #\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), \n",
    "                                               batch_size=batch_size, class_mode='categorical')\n",
    "val_data = val_datagen.flow_from_directory(val_dir, target_size=(224, 224), \n",
    "                                           batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Function to Create VGG16 Model\n",
    "# --------------------------- #\n",
    "def create_vgg16_model():\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False  # Freeze pre-trained layers\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(NUM_CLASSES, activation=\"softmax\")(x)  # Ensure correct number of classes\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Train and Save VGG16 Model (if not exists)\n",
    "# --------------------------- #\n",
    "vgg16_model_path = \"vgg16_model.h5\"\n",
    "\n",
    "if not os.path.exists(vgg16_model_path):\n",
    "    print(\"Training VGG16 Model... ðŸš€\")\n",
    "    vgg16_model = create_vgg16_model()\n",
    "    vgg16_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                        loss=\"categorical_crossentropy\",\n",
    "                        metrics=[\"accuracy\"])\n",
    "    \n",
    "    vgg16_model.fit(train_data, epochs=30, validation_data=val_data)\n",
    "    vgg16_model.save(vgg16_model_path)\n",
    "    print(f\"âœ… VGG16 Model saved as '{vgg16_model_path}'.\")\n",
    "else:\n",
    "    print(f\"âœ… Found existing model '{vgg16_model_path}', skipping training.\")\n",
    "\n",
    "# Load the trained model\n",
    "vgg16_model = create_vgg16_model()\n",
    "vgg16_model.load_weights(vgg16_model_path)\n",
    "print(\"âœ… VGG16 Model Loaded Successfully!\")\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Function: Preprocess Image for VGG16\n",
    "# --------------------------- #\n",
    "def preprocess_for_vgg16(image):\n",
    "    image = image.resize((224, 224))  # Resize image\n",
    "    image = np.array(image) / 255.0  # Normalize\n",
    "    image = np.expand_dims(image, axis=0)  # Expand dims for model\n",
    "    return image\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Function: Classify Image Using VGG16\n",
    "# --------------------------- #\n",
    "def classify_image(image_path):\n",
    "    image = Image.open(image_path)  # Open image\n",
    "    processed_img = preprocess_for_vgg16(image)  # Preprocess for VGG16\n",
    "    predictions = vgg16_model.predict(processed_img)  # Predict\n",
    "\n",
    "    class_id = np.argmax(predictions)\n",
    "    confidence = predictions[0][class_id]\n",
    "    \n",
    "    # Class labels\n",
    "    class_labels = ['10_new', '20_new', '50_new', '100_new', '200_new','500_new']\n",
    "    label = f\"{class_labels[class_id]} ({confidence:.2f})\"\n",
    "    \n",
    "    return label\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Function to Open File Dialog and Select Image\n",
    "# --------------------------- #\n",
    "def select_image():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    image_path = filedialog.askopenfilename(title=\"Select Image\", filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png\")])\n",
    "    return image_path\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Function to Display the Classification Result\n",
    "# --------------------------- #\n",
    "def show_classification_result(image_path):\n",
    "    result_label.config(text=\"Processing... Please wait.\", fg=\"black\")\n",
    "    \n",
    "    # Run classification in a background thread to avoid blocking UI\n",
    "    threading.Thread(target=run_classification, args=(image_path,)).start()\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Function to Run Classification in the Background\n",
    "# --------------------------- #\n",
    "def run_classification(image_path):\n",
    "    try:\n",
    "        # Classify the uploaded image\n",
    "        result = classify_image(image_path)\n",
    "        \n",
    "        # Update the result label in the main thread\n",
    "        result_label.config(text=f\"Prediction: {result}\", fg=\"green\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # In case of error, display the error message\n",
    "        result_label.config(text=f\"Error: {str(e)}\", fg=\"red\")\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Function to Load and Display Image in Tkinter\n",
    "# --------------------------- #\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image.thumbnail((400, 400))  # Resize for better display\n",
    "    img_tk = ImageTk.PhotoImage(image)\n",
    "    \n",
    "    # Keep a reference of the image to prevent it from being garbage collected\n",
    "    preview_label.image = img_tk  \n",
    "    preview_label.config(image=img_tk)\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Create the Tkinter GUI\n",
    "# --------------------------- #\n",
    "window = tk.Tk()\n",
    "window.title(\"Image Classification\")\n",
    "window.geometry(\"600x600\")\n",
    "\n",
    "# Create an upload button\n",
    "upload_button = tk.Button(window, text=\"Upload Image\", font=(\"Helvetica\", 14), command=lambda: on_upload())\n",
    "upload_button.pack(pady=10)\n",
    "\n",
    "# Label to display the selected image preview\n",
    "preview_label = tk.Label(window)\n",
    "preview_label.pack(pady=20)\n",
    "\n",
    "# Label for the classification result\n",
    "result_label = tk.Label(window, text=\"Prediction: None\", font=(\"Helvetica\", 14), fg=\"blue\")\n",
    "result_label.pack(pady=20)\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Upload Button Logic\n",
    "# --------------------------- #\n",
    "def on_upload():\n",
    "    image_path = select_image()\n",
    "    if image_path:\n",
    "        load_image(image_path)\n",
    "        show_classification_result(image_path)\n",
    "    else:\n",
    "        classes=['10_new', '20_new', '50_new', '100_new', '200_new','500_new'] \n",
    "        messagebox.showerror(\"Error\", \"No image selected. Please upload an image.\")\n",
    "\n",
    "# Run the Tkinter event loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaed65f-3662-4e83-8f8d-41f9adec8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "yolo_model = YOLO(\"yolov8s.pt\")\n",
    "# Configure model to only detect currency notes\n",
    "yolo_model.classes = [67]  # Class ID for currency notes\n",
    "\n",
    "# --------------------------- #\n",
    "# ðŸ”¹ Load VGG16 Model for Currency Classification\n",
    "# --------------------------- #\n",
    "def create_vgg16_model():\n",
    "    input_shape = (224, 224, 3)\n",
    "    base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(5, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "vgg16_model = create_vgg16_model()\n",
    "vgg16_model.load_weights(\"vgg16_model.h5\")\n",
    "\n",
    "# Class labels for currency notes\n",
    "class_labels = ['10_new', '20_new', '50_new', '100_new', '200_new','500_new']\n",
    "\n",
    "def preprocess_for_vgg16(cropped_img):\n",
    "    if cropped_img.size == 0:\n",
    "        return None\n",
    "    try:\n",
    "        cropped_img = cv2.resize(cropped_img, (224, 224))\n",
    "        cropped_img = cropped_img.astype(\"float32\") / 255.0\n",
    "        cropped_img = np.expand_dims(cropped_img, axis=0)\n",
    "        return cropped_img\n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing: {e}\")\n",
    "        return None\n",
    "\n",
    "def is_valid_currency_note(box, conf, min_area=5000):\n",
    "    # Get box dimensions\n",
    "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "    aspect_ratio = (x2 - x1) / (y2 - y1)\n",
    "    \n",
    "    # Check if the note meets our criteria\n",
    "    return (conf > 0.6 and  # High confidence\n",
    "            area >= min_area and  # Minimum size\n",
    "            1.5 <= aspect_ratio <= 3.0)  # Typical currency note aspect ratio\n",
    "\n",
    "def real_time_detection_and_classification():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Set camera properties for better performance\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "    print(\"Starting detection. Press 'q' to quit.\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Create a copy for display\n",
    "        display_frame = frame.copy()\n",
    "\n",
    "        # Run YOLO detection\n",
    "        results = yolo_model(frame, conf=0.5)\n",
    "\n",
    "        best_note = None\n",
    "        highest_conf = 0\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            for box in boxes:\n",
    "                conf = float(box.conf[0])\n",
    "                \n",
    "                # Check if this is a valid currency note with higher confidence\n",
    "                if is_valid_currency_note(box, conf):\n",
    "                    if conf > highest_conf:\n",
    "                        highest_conf = conf\n",
    "                        best_note = box\n",
    "\n",
    "        # Process only the best detected note\n",
    "        if best_note is not None:\n",
    "            x1, y1, x2, y2 = map(int, best_note.xyxy[0])\n",
    "            cropped_obj = frame[y1:y2, x1:x2]\n",
    "            processed_img = preprocess_for_vgg16(cropped_obj)\n",
    "\n",
    "            if processed_img is not None:\n",
    "                try:\n",
    "                    predictions = vgg16_model.predict(processed_img, verbose=0)\n",
    "                    note_class = np.argmax(predictions)\n",
    "                    note_conf = predictions[0][note_class]\n",
    "\n",
    "                    if note_conf > 0.7:  # High confidence threshold for currency\n",
    "                        label = f\"{class_labels[note_class]} ({note_conf:.2f})\"\n",
    "                        cv2.rectangle(display_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        cv2.putText(display_frame, label, (x1, y1 - 10),\n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in classification: {e}\")\n",
    "\n",
    "        # Add FPS counter\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        cv2.putText(display_frame, f\"FPS: {int(fps)}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow(\"Currency Detection\", display_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    real_time_detection_and_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a48c4-9c87-4d81-a7d9-f17f3cf997de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
